{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arghavan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Happiness DF\n",
    "Happiness =  pd.read_csv('Resources/WorldHappinessReport.csv')\n",
    "Happiness_New = Happiness.drop(columns=['Delivery Quality', 'Democratic Quality', \n",
    "                                        'Standard deviation of ladder by country-year',\n",
    "                                        'Standard deviation/Mean of ladder by country-year',\n",
    "                                        'GINI index (World Bank estimate)', 'GINI index (World Bank estimate), average 2000-15',\n",
    "                                        'gini of household income reported in Gallup, by wp5-year'])\n",
    "\n",
    "Happiness_Updated = Happiness_New.loc[Happiness_New[\"year\"]==2016,:]\n",
    "Happiness_Olympics = Happiness_Updated.dropna()\n",
    "Happiness_Olympics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Olympics DF\n",
    "CSV_athletes = pd.read_csv(\"Resources/120-years-of-olympic-history-athletes-and-results/athlete_events.csv\")\n",
    "CSV_countries =  pd.read_csv(\"Resources/120-years-of-olympic-history-athletes-and-results/noc_regions.csv\")\n",
    "CSV_athletes = CSV_athletes.drop(columns=['Season', 'City', 'Height', 'Weight'])\n",
    "CSV_athletes = CSV_athletes.loc[CSV_athletes[\"Year\"]==2016,:]\n",
    "CSV_athletes_updated = CSV_athletes.fillna(\"none\")\n",
    "merged_athletes = pd.merge(CSV_athletes_updated, CSV_countries, on = \"NOC\", how = \"left\")\n",
    "merged_athletes = merged_athletes.drop(columns = ['NOC', 'notes'])\n",
    "merged_athletes1 = merged_athletes.rename(columns= {\"region\" : \"country\"})\n",
    "\n",
    "merged_athletes2 = merged_athletes1.groupby(merged_athletes1[\"country\"])\n",
    "Final_athletes = merged_athletes2[\"Name\"].count()\n",
    "Final_df = pd.DataFrame(Final_athletes)\n",
    "Final_df = Final_df.rename(columns= {\"Name\" : \"Number of Athletes\"})\n",
    "Final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Final DF\n",
    "Athlete_Happiness = pd.merge(Final_df, Happiness_Olympics, on = \"country\")\n",
    "Athlete_Happiness.to_csv(\"Outputs/Athlete_Happiness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shayan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "happiness_df = pd.read_csv(\"Resources/WorldHappinessReport.csv\")\n",
    "tourism_df = pd.read_csv(\"Resources/Tourism.csv\", skiprows=3)\n",
    "happiness17_df = happiness_df.loc[(happiness_df[\"year\"] == 2017)]\n",
    "happiness_score = happiness17_df[[\"country\", \"Life Ladder\"]]\n",
    "happiness_score.columns = [\"Country Name\", \"Happines Score\"]\n",
    "tourism_number = tourism_df[[\"Country Name\", \"2017\"]]\n",
    "tourism_number = tourism_number.fillna(0)\n",
    "merge_table = pd.merge(happiness_score, tourism_number, on = \"Country Name\")\n",
    "merge_table.to_csv(\"Outputs/Tourism_Happiness.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mariana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from a file using pandas\n",
    "file = 'Resources/Capitals.csv'\n",
    "# read the CSV in pandas\n",
    "weather = pd.read_csv(file, encoding=\"ISO-8859-1\")\n",
    "weather.reset_index(inplace=True)\n",
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up additional columns to hold information\n",
    "weather['Temperature'] = \"\"\n",
    "\n",
    "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "\n",
    "#print(response['main']['temp'])\n",
    "# use iterrows to iterate through pandas dataframe\n",
    "for index, row in weather.iterrows():\n",
    "\n",
    "    # iterate thru each row of the file\n",
    "    city = row['Cities']\n",
    "    url = base_url + \"appid=\" + \"082ff2cde1d396b2b896b2335daad199\" + \"&q=\" + city +\"&units=imperial\"\n",
    "\n",
    "# assemble url and make API request\n",
    "    response = requests.get(url).json()\n",
    "  \n",
    "    #update the dataframe \n",
    "    try:\n",
    "        weather.loc[index, 'Temperature'] = response['main']['temp']\n",
    "        \n",
    "    except (KeyError, IndexError):\n",
    "        print(\"Missing field/result... skipping.\")\n",
    "#         print(response['main']['temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get top songs by country choosing one random day a month for the whole year of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from a file using pandas\n",
    "file = 'Resources/CountryCodesLookup.csv'\n",
    "#upload country lookup\n",
    "Countries=pd.read_csv(file,encoding=\"ISO-8859-1\")\n",
    "#upload dates to look for top songs\n",
    "spDates= pd.read_csv('Resources/DatesForSpotifyTopSongs.csv')\n",
    "Countries.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by countries in spotify\n",
    "spCountries = Countries.loc[Countries[\"spotifyFlag\"] == 1,[\"COUNTRY\",\"A2 (ISO)\"]]\n",
    "\n",
    "# make a list of countries by dates to get their top 200 songs\n",
    "#cross join\n",
    "spCountries[\"Key\"]=1\n",
    "spDates[\"Key\"]=1\n",
    "spCountries[\"Processed\"]=0\n",
    "CountryDate = pd.merge(spCountries, spDates, on=\"Key\").reset_index(drop=True)\n",
    "#.head(1)\n",
    "#CountryDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract top songs by country in 2017. \n",
    "# rows get marked as processed. If the script failes, at re-start, it will exclude processed/failed rows.\n",
    "# this needs to be restarted as many times as it fails.\n",
    "\n",
    "spCountriesToProcess = CountryDate.loc[(CountryDate[\"Processed\"] == 0) & (CountryDate[\"COUNTRY\"]!='AD' ),:]\n",
    "\n",
    "\n",
    "from fycharts import SpotifyCharts \n",
    "api = SpotifyCharts.SpotifyCharts()\n",
    "\n",
    "\n",
    "# use iterrows to iterate through pandas dataframe\n",
    "for index, row in spCountriesToProcess.iterrows():\n",
    "\n",
    "    # iterate thru each row of the file\n",
    "    countryCode = str.lower(row['A2 (ISO)'])\n",
    "    startDate=row[\"startDate\"]\n",
    "    EndDate=row[\"EndDate\"]\n",
    "    \n",
    "    #get data\n",
    "    print(startDate)  \n",
    "    print (countryCode)\n",
    "    CountryDate.loc[index, 'Processed'] = 1\n",
    "   \n",
    "       \n",
    "    api.top200Daily(output_file = 'Outputs/Spotify2017Top.csv', start=startDate, end=EndDate, region=countryCode)\n",
    "#viral50Daily    \n",
    "#top200Weekly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Query spotify API. Find the songs attributes.  Classify all songs found and saved in the Soptify2017Top.cvs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='Outputs/Spotify2017Top1.csv'\n",
    "TopSpotify=pd.read_csv(file,encoding=\"ISO-8859-1\")\n",
    "TopSpotify.columns\n",
    "TopSpotify[\"uri\"]='spotify:track:'+TopSpotify[\"id\"]\n",
    "#New columns\n",
    "TopSpotify[\"happiness\"]=''\n",
    "TopSpotify[\"energy\"]=''\n",
    "TopSpotify[\"danceability\"]=''\n",
    "TopSpotify[\"loudness\"]=''\n",
    "TopSpotify[\"speechiness\"]=''\n",
    "TopSpotify[\"instrumentalness\"]=''\n",
    "\n",
    "\n",
    "TopSpotify.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spotipy\n",
    "\n",
    "client_id='hiddenCode'\n",
    "client_secret='HiddenCode'\n",
    "\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id,client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "\n",
    "\n",
    "# use iterrows to iterate through pandas dataframe\n",
    "for index, row in TopSpotify.iterrows():\n",
    "\n",
    "    # iterate thru each row of the file\n",
    "    tid = row['uri']\n",
    "    try:\n",
    "        fearure = sp.audio_features(tid)\n",
    "        happiness = fearure[0][\"valence\"]\n",
    "        energy = fearure[0][\"energy\"]\n",
    "        danceability=fearure[0][\"danceability\"]\n",
    "        loudness=fearure[0][\"loudness\"]\n",
    "        speechiness=fearure[0][\"speechiness\"]\n",
    "        instrumentalness=fearure[0][\"instrumentalness\"]\n",
    "\n",
    "    #write back to data frame\n",
    "               \n",
    "        TopSpotify.loc[index, 'happiness'] = happiness\n",
    "        TopSpotify.loc[index, 'energy'] = energy\n",
    "        TopSpotify.loc[index, 'danceability'] = danceability\n",
    "        TopSpotify.loc[index, 'loudness'] = loudness\n",
    "        TopSpotify.loc[index, 'speechiness'] = speechiness\n",
    "        TopSpotify.loc[index, 'instrumentalness'] = instrumentalness\n",
    "        \n",
    "\n",
    "    except (KeyError, IndexError, NoneType):\n",
    "        print(tid)\n",
    "        print(\"Missing field/result... skipping.\")\n",
    "TopSpotify.to_csv(\"Outputs/SpotifyTop2017Final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
